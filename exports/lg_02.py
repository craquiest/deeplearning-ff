
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: deeplearning-ff/02_fully_connected.ipynb

from exports.lg_01 import *

def get_data():
    path = datasets.download_data(MNIST_URL, ext='.gz')
    with gzip.open(path, 'rb') as f:
        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')
    return map(tensor, (x_train,y_train,x_valid,y_valid))

def normalize(x, m, s): return (x-m)/s

def test_near_zero(a,tol=1e-3): assert a.abs()<tol, f"Near zero: {a}"

def lin_man(x, w, b):
  """Manual linear transform"""
  return x@w + b

def kaiming_init(m,nh):
  """Scratch Kaiming He init. Use as w1 = kaiming_init(m,nh)"""
  return torch.randn(m,nh)*math.sqrt(2./m)

from torch.nn import init

def relu_tweak(x):
  """Tweaked ReLU: what if we substract 0.5 to get relu mean to 0"""
  return x.clamp_min(0.) - 0.5

def mse(output, targ): return (output.squeeze(-1) - targ).pow(2).mean()

def mse_grad(inp, targ):
    # grad of loss with respect to output of previous layer;
    # inp here is 'out' of relu; so we store the grad there to pass it through, and use chain rule
    # this is dMSE / dyhat
    inp.g = 2. * (inp.squeeze() - targ).unsqueeze(-1) / inp.shape[0]
    # 2/N (yhat-y); yhat is inp, inp/shape is N (numb classes/dim output)

def relu_grad(inp, out):
    # grad of relu with respect to input activations
    # this is dRelu(z)/dz * dMSE/dRelu: accumulating chain rule
    inp.g = (inp>0).float() * out.g
    # out.g contains inp.g from mse_grad stored by mse_grad
    # inp.g is stored for lin_grad

def lin_grad(inp, out, w, b):
    # grad of matmul with respect to input,weight and biase
    inp.g = out.g @ w.t()     # grad of matrix prod = matri prod with transpose
    # Creating a giant outer product, just to sum it, is inefficient!
    w.g = (inp.unsqueeze(-1) * out.g.unsqueeze(1)).sum(0) #sum over training examples (1/m baked in from mse_grad)
    b.g = out.g.sum(0) #sum over training examples (1/m baked in from mse_grad)

def forward_and_backward(inp, targ):
    # forward pass:
    l1 = inp @ w1 + b1
    l2 = relu_tweak(l1)
    out = l2 @ w2 + b2
    # we don't actually need the loss in backward!
    loss = mse(out, targ)

    # backward pass:
    mse_grad(out, targ)
    lin_grad(l2, out, w2, b2) # out.g contains grad of next layer
    relu_grad(l1, l2)         # l2.g contains grad of next layer
    lin_grad(inp, l1, w1, b1) # l1.g contains grad of next layer

from torch import nn