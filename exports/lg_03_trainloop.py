
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: deeplearning-ff/03_minibatch_training.ipynb

from exports.lg_02 import *

from torch import optim
import torch.nn.functional as F
from torch.utils.data import DataLoader, SequentialSampler, RandomSampler


def log_softmax(x):
    # in pytorch, neg log likelihood expect a softmax
    return (x.exp()/(x.exp().sum(-1,keepdim=True))).log()

def nll(input, target):
    # expects log softmax, not softmx
    # u are zippinng the indices of rows, and the looked up column pos of target in the sm_pred matrix
  """Negative log-likelihood"""
  return -input[range(target.shape[0]), target].mean()

def logsumexp(x):
    m = x.max(-1)[0]
    return m + (x-m[:,None]).exp().sum(-1).log()

def log_softmax_refact(x):
  """Re-factored log softmax"""
  return x - x.logsumexp(-1,keepdim=True)

def cross_entropy_loss(pred,target):
  """Our manual cross-entropy loss"""
  return nll(log_softmax_refact(pred), target)

def accuracy(pred, target):
  """What percentage did we get righht? Remember to float before mean"""
  return (torch.argmax(pred, dim=1)==target).float().mean()

class Model(nn.Module):
    """basic model from lesson 8, with pytorch  pieces"""
    def __init__(self, n_in, nh, n_out):
        super().__init__()
        self.l1 = nn.Linear(n_in,nh)
        self.l2 = nn.Linear(nh,n_out)

    def __call__(self, x): 
        # this is what shows when u call instance like a function (instance not Class)
        return self.l2(F.relu(self.l1(x)))

def fit():
    """basic training loop
    >>> model =...
    >>> loss_func = ...
    >>> fit()
    """
    for epoch in range(epochs):
        for i in range((n-1)//bs + 1):
            start_i = i*bs
            end_i = start_i+bs
            xb = x_train[start_i:end_i]
            yb = y_train[start_i:end_i]
            loss = loss_func(model(xb), yb)

            loss.backward()
            with torch.no_grad():
                for p in model.parameters(): p -= p.grad * lr
                model.zero_grad()

class DummyModule():
    """Module class Mechanics to get all params quick
    >>> mdl = DummyModule(m,nh,10)
    >>> mdl
    >>> [o.shape for o in mdl.parameters()]
    """
    def __init__(self, n_in, nh, n_out):
        self._modules = {}
        self.l1 = nn.Linear(n_in,nh)
        self.l2 = nn.Linear(nh,n_out)

    def __setattr__(self,k,v):
        if not k.startswith("_"): self._modules[k] = v
        super().__setattr__(k,v)

    def __repr__(self): return f'{self._modules}'
    # this is what shows when u evaluate instance by name, no parenthesis

    def parameters(self):
        for l in self._modules.values():
            for p in l.parameters(): yield p

class SequentialModel(nn.Module):
    """Manual version of nn.Sequential
    >>> layers = [nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10)]
    >>> model = SequentialModel(layers)
    """
    def __init__(self, layers):
        super().__init__()
        self.layers = nn.ModuleList(layers)

    def __call__(self, x):
        for l in self.layers: x = l(x)
        return x

class Optimizer():
    """Our manual optimizer
    >>> model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))
    >>> opt = Optimizer(model.parameters())
    inside loop in fit():
      opt.step()
      opt.zero_grad()
    """
    def __init__(self, params, lr=0.5):
      self.params,self.lr=list(params),lr

    def step(self):
        with torch.no_grad():
            for p in self.params: p -= p.grad * lr

    def zero_grad(self):
        for p in self.params: p.grad.data.zero_()


def get_model():
    """Get model and optimizer in one go
    model,opt = get_model()
    """
    model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))
    return model, optim.SGD(model.parameters(), lr=lr)

class Dataset():
    """"Manual Dataset class
    >> train_ds,valid_ds = Dataset(x_train, y_train),Dataset(x_valid, y_valid)
    """
    def __init__(self, x, y): self.x,self.y = x,y
    def __len__(self): return len(self.x) # lets you do len(obg) in Python once defined
    def __getitem__(self, i): return self.x[i],self.y[i] # enables indexing

class DataLoader_man():
    """Our manual Dataloader, lets you iterate thru batches
    >> train_dl = DataLoader_man(train_ds, bs)
    >> valid_dl = DataLoader_man(valid_ds, bs)
    """
    def __init__(self, ds, bs): self.ds,self.bs = ds,bs
    def __iter__(self):
        # this is what happens when u day "for xxx in dataloader_instance"
        for i in range(0, len(self.ds), self.bs): yield self.ds[i:i+self.bs]

def fit_refact():
    """Refactored trainng loop"""
    for epoch in range(epochs):
        for xb,yb in train_dl:
            pred = model(xb)
            loss = loss_func(pred, yb)
            loss.backward()
            opt.step()
            opt.zero_grad()

class Sampler():
    """ Sampler to shuffle when data-loading
    train_samp = Sampler(train_ds, bs, shuffle=True)
    valid_samp = Sampler(valid_ds, bs, shuffle=False)
    train_dl = DataLoader_wSampler(train_ds, sampler=train_samp, collate_fn=collate)
    valid_dl = DataLoader_wSampler(valid_ds, sampler=valid_samp, collate_fn=collate)
    """
    def __init__(self, ds, bs, shuffle=False):
        #stores just the length to shuffle, the batch-size, and whether to shuffle
        self.n,self.bs,self.shuffle = len(ds),bs,shuffle

    def __iter__(self):
        self.idxs = torch.randperm(self.n) if self.shuffle else torch.arange(self.n)
        for i in range(0, self.n, self.bs): yield self.idxs[i:i+self.bs] # yield indexes

def collate(b):
    xs,ys = zip(*b)
    return torch.stack(xs),torch.stack(ys)

class DataLoader_wSampler():
    def __init__(self, ds, sampler, collate_fn=collate):
        self.ds,self.sampler,self.collate_fn = ds,sampler,collate_fn

    def __iter__(self):
        for s in self.sampler: yield self.collate_fn([self.ds[i] for i in s]) # yield with shuffled indices


def fit_print(epochs, model, loss_func, opt, train_dl, valid_dl):
    """Training loop with printing avg_losses and avg_accuracy"""
    for epoch in range(epochs):
        # Handle batchnorm / dropout; only when training
        model.train() # go in training mode, sets model.training to True
        #print(model.training)
        for xb,yb in train_dl:
            loss = loss_func(model(xb), yb)
            loss.backward()
            opt.step()
            opt.zero_grad()

        model.eval() #go in eval mode, sets model.training to False
        #print(model.training)
        with torch.no_grad():
            tot_loss,tot_acc = 0.,0.
            for xb,yb in valid_dl:
                pred = model(xb)
                tot_loss += loss_func(pred, yb)
                tot_acc  += accuracy (pred,yb)
        nv = len(valid_dl)
        print(epoch, tot_loss/nv, tot_acc/nv) #weight avg is more correct for acc. tbd
    return tot_loss/nv, tot_acc/nv

def get_dls(train_ds, valid_ds, bs, **kwargs):
    """train_dl,valid_dl = get_dls(train_ds, valid_ds, bs)"""
    return (DataLoader(train_ds, batch_size=bs, shuffle=True, **kwargs),
            DataLoader(valid_ds, batch_size=bs*2, **kwargs))