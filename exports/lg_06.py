
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: deeplearning-ff/06_cuda_cnn_hooks_init.ipynb

from exports.lg_05b import *
torch.set_num_threads(2)

def normalize_to(train, valid):
    m,s = train.mean(),train.std()
    return normalize(train, m, s), normalize(valid, m, s)

class Lambda(nn.Module):
    def __init__(self, func):
        super().__init__()
        self.func = func

    def forward(self, x): return self.func(x)

def flatten(x):
  """keeps 1st dim (batch size) and blends all other dims by multiplying them """
  return x.view(x.shape[0], -1)

class CudaCallback(Callback):
    def begin_fit(self): self.model.cuda()
    def begin_batch(self): self.run.xb,self.run.yb = self.xb.cuda(),self.yb.cuda()

class CpuCallback(Callback):
    def begin_fit(self):
      # model.to() modifies the model in place
      self.model.to("cpu")
    def begin_batch(self):
      # tensor.to creates a copy, so you need to assign result to original tensors
      self.run.xb,self.run.yb = self.xb.to("cpu"),self.yb.to("cpu")

class BatchTransformXCallback(Callback): # replaces Lambda(mnist)
    _order=2
    def __init__(self, tfm): self.tfm = tfm
    def begin_batch(self): self.run.xb = self.tfm(self.xb)#transforms x not in model but at batch time

def view_tfm(*size):
    def _inner(x): return x.view(*((-1,)+size))
    return _inner

def get_runner(model, data, lr=0.6, cbs=None, opt_func=None, loss_func = F.cross_entropy):
    if opt_func is None: opt_func = optim.SGD
    opt = opt_func(model.parameters(), lr=lr)
    learn = Learner(model, opt, loss_func, data)
    return learn, Runner(cb_funcs=listify(cbs))

class SequentialModel(nn.Module):
    """"manual Sequential model 2.0, with tracking, telemetry """
    def __init__(self, *layers):
        super().__init__()
        self.layers = nn.ModuleList(layers)
        self.act_means = [[] for _ in layers] # each layer has list
        self.act_stds  = [[] for _ in layers]

    def __call__(self, x):
        for i,l in enumerate(self.layers):
            x = l(x)
            # each list gets added to every time fwd pass computed, so each batch
            self.act_means[i].append(x.data.mean())
            self.act_stds [i].append(x.data.std ())
        return x

    def __iter__(self):
      #when class looped on, loop on the layers
      return iter(self.layers)

def children(m): return list(m.children())

class Hook():
    """manual implementation of fastai hooks, only for forward"""
    def __init__(self, m, f): self.hook = m.register_forward_hook(partial(f, self)) # passes itself to f
    def remove(self): self.hook.remove() # need to remove hook
    def __del__(self): self.remove() # when python cleans object, obj needs to get remove itsel first

def append_stats(hook, mod, inp, outp):
    # this is going to be the f passed to Hook.__init__
    # and partial makes sure its a 3 arg func, while giving it access to hook itslef
    if not hasattr(hook,'stats'): hook.stats = ([],[]) #first time, we create stats lists in hook
    means,stds = hook.stats # get each list
    means.append(outp.data.mean()) # # add to them
    stds .append(outp.data.std())

class ListContainer():
    def __init__(self, items): self.items = listify(items)
    def __getitem__(self, idx):
        if isinstance(idx, (int,slice)): return self.items[idx]
        if isinstance(idx[0],bool):
            assert len(idx)==len(self) # bool mask, need 1 bool for each item
            return [o for m,o in zip(idx,self.items) if m] # take only Trues
        return [self.items[i] for i in idx] # only ones in idx (int, slice)
    def __len__(self): return len(self.items)
    def __iter__(self): return iter(self.items)
    def __setitem__(self, i, o): self.items[i] = o
    def __delitem__(self, i): del(self.items[i])
    def __repr__(self):
        res = f'{self.__class__.__name__} ({len(self)} items)\n{self.items[:10]}'
        if len(self)>10: res = res[:-1]+ '...]' #:-1 takes off ]
        return res

from torch.nn import init

class Hooks(ListContainer):
    def __init__(self, ms, f): super().__init__([Hook(m, f) for m in ms])
    def __enter__(self, *args): return self # when getting in context manager
    def __exit__ (self, *args): self.remove() # when get out of context manager
    def __del__(self): self.remove() # when python clean, trigger all hooks removal

    def __delitem__(self, i):
        self[i].remove() #remove any intem then let it get deleted delete it
        super().__delitem__(i) #do what u dothen call super, not reverse

    def remove(self):
        # this is where ListContainer.__iter__ gets used 'for h in self'
        for h in self: h.remove() # remove hooks one y one

def append_stats_1(hook, mod, inp, outp):
    if not hasattr(hook,'stats'): hook.stats = ([],[],[])
    means,stds,hists = hook.stats
    means.append(outp.data.mean().cpu())
    stds .append(outp.data.std().cpu())
    hists.append(outp.data.cpu().histc(40,0,10)) #histc isn't implemented on the GPU

# Thanks to @ste for initial version of histgram plotting code; log(1+x) to avoid zeros
def get_hist(h): return torch.stack(h.stats[2]).t().float().log1p()

def get_cnn_layers(data, nfs, layer, **kwargs): # **kwargs go to GeneralRelu
    nfs = [1] + nfs
    return [layer(nfs[i], nfs[i+1], 5 if i==0 else 3, **kwargs) #i==0
            for i in range(len(nfs)-1)] + [
        nn.AdaptiveAvgPool2d(1), Lambda(flatten), nn.Linear(nfs[-1], data.c)]

def conv_layer(ni, nf, ks=3, stride=2, **kwargs):# **kwargs go to GeneralRelu
    return nn.Sequential(
        nn.Conv2d(ni, nf, ks, padding=ks//2, stride=stride), GeneralRelu(**kwargs))

class GeneralRelu(nn.Module):
    def __init__(self, leak=None, sub=None, maxv=None):
        super().__init__()
        self.leak,self.sub,self.maxv = leak,sub,maxv

    def forward(self, x):
        x = F.leaky_relu(x,self.leak) if self.leak is not None else F.relu(x)
        if self.sub is not None: x.sub_(self.sub) # minus 0.5 e.g.
        if self.maxv is not None: x.clamp_max_(self.maxv) # cap
        return x

def init_cnn(m, uniform=False,a=0.1): # make a variable
    f = init.kaiming_uniform_ if uniform else init.kaiming_normal_
    for l in m:
        if isinstance(l, nn.Sequential):
            f(l[0].weight, a=a) # vary a i.o fixed 0.1
            l[0].bias.data.zero_()

def get_cnn_model(data, nfs, layer, **kwargs):# **kwargs go to GeneralRelu
    return nn.Sequential(*get_cnn_layers(data, nfs, layer, **kwargs))

def append_stats_2(hook, mod, inp, outp):
    if not hasattr(hook,'stats'): hook.stats = ([],[],[])
    means,stds,hists = hook.stats
    means.append(outp.data.mean().cpu())
    stds .append(outp.data.std().cpu())
    hists.append(outp.data.cpu().histc(40,-7,7))

def get_learn_run(nfs, data, lr, layer, cbs=None, opt_func=None, uniform=False, a=0.1, **kwargs): # add a var for kaimin
    model = get_cnn_model(data, nfs, layer, **kwargs)
    init_cnn(model, uniform=uniform, a=a)
    return get_runner(model, data, lr=lr, cbs=cbs, opt_func=opt_func)

from IPython.display import display, Javascript
def nb_auto_export():
    display(Javascript("""{
const ip = IPython.notebook
if (ip) {
    ip.save_notebook()
    console.log('a')
    const s = `!python notebook2script.py ${ip.notebook_name}`
    if (ip.kernel) { ip.kernel.execute(s) }
}
}"""))