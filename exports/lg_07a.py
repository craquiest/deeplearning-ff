
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: deeplearning-ff/07a_lsuv.ipynb

from exports.lg_07 import *

class ConvLayer(nn.Module):
    def __init__(self, ni, nf, ks=3, stride=2, sub=0., **kwargs):
        super().__init__()
        self.conv = nn.Conv2d(ni, nf, ks, padding=ks//2, stride=stride, bias=True)
        self.relu = GeneralRelu(sub=sub, **kwargs)

    def forward(self, x): return self.relu(self.conv(x))

    @property
    def bias(self): return -self.relu.sub
    @bias.setter
    def bias(self,v): self.relu.sub = -v
    @property
    def weight(self): return self.conv.weight

def get_batch(dl, run):
    run.xb,run.yb = next(iter(dl))
    for cb in run.cbs: cb.set_runner(run)
    run('begin_batch')
    return run.xb,run.yb

def find_modules(m, cond):
    """Find recursively modules satisfying a certain condition
    define condition as function, or lambda function
    >> mods = find_modules(learn.model, lambda o: isinstance(o,ConvLayer))"""
    if cond(m): return [m]
    return sum([find_modules(o,cond) for o in m.children()], [])

def is_lin_layer(l):
    """Condition function to find linear family layers"""
    lin_layers = (nn.Conv1d, nn.Conv2d, nn.Conv3d, nn.Linear, nn.ReLU)
    return isinstance(l, lin_layers)

def lsuv_module(m, xb):
    """Apply LSUV init to each individual module using a hook
    Do with one batch, adjust bias and weight till you get 0 mean, 1 stdev
    >> for m in mods: print(lsuv_module(m, xb)) """
    h = Hook(m, append_stat)

    while mdl(xb) is not None and abs(h.mean)  > 1e-3:
      m.bias -= h.mean # use bias property in ConLayer
    while mdl(xb) is not None and abs(h.std-1.0) > 1e-3:
      m.weight.data /= h.std # use weight property in ConLayer

    h.remove() # dont forget to remove hook
    return h.mean,h.std