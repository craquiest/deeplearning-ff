
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: deeplearning-ff/07_batchnorm.ipynb

from exports.lg_06 import *

class BatchNorm(nn.Module):
    def __init__(self, nf, mom=0.1, eps=1e-5):
        super().__init__()
        # NB: pytorch bn mom is opposite of what you'd expect
        self.mom,self.eps = mom,eps
        self.mults = nn.Parameter(torch.ones (nf,1,1)) # learnable
        self.adds  = nn.Parameter(torch.zeros(nf,1,1)) # learnable
        self.register_buffer('vars',  torch.ones(1,nf,1,1)) # if we move model to gpu, will move along
        self.register_buffer('means', torch.zeros(1,nf,1,1)) # saved allong with model, to use at inference

    def update_stats(self, x):
        m = x.mean((0,2,3), keepdim=True) # avg across batch, H and W;only channel dimension left
        v = x.var ((0,2,3), keepdim=True) # but keepdim, so stays in shape with lots of same numbers
        self.means.lerp_(m, self.mom) # running avg, using just last two
        self.vars.lerp_ (v, self.mom) # 'linear interp', here mom * now + (1-mom)* past
        return m,v

    def forward(self, x):
        if self.training:#normalize with current stats during traing
            with torch.no_grad(): m,v = self.update_stats(x)
        else: m,v = self.means,self.vars # just use running avg at inference
        x = (x-m) / (v+self.eps).sqrt() # normalize
        return x*self.mults + self.adds # scale and shift, not more need for bias in cov layers

def init_cnn_(m, f): # init recursively
    if isinstance(m, nn.Conv2d):
        f(m.weight, a=0.1)
        if getattr(m, 'bias', None) is not None: m.bias.data.zero_()
    for l in m.children(): init_cnn_(l, f) # init recursively

def init_cnn(m, uniform=False):
    f = init.kaiming_uniform_ if uniform else init.kaiming_normal_
    init_cnn_(m, f)

def get_learn_run(nfs, data, lr, layer, cbs=None, opt_func=None, uniform=False, **kwargs):
    model = get_cnn_model(data, nfs, layer, **kwargs)
    init_cnn(model, uniform=uniform)
    return get_runner(model, data, lr=lr, cbs=cbs, opt_func=opt_func)

def conv_layer(ni, nf, ks=3, stride=2, bn=True, **kwargs):
    layers = [nn.Conv2d(ni, nf, ks, padding=ks//2, stride=stride, bias=not bn),
              GeneralRelu(**kwargs)]
    if bn: layers.append(nn.BatchNorm2d(nf, eps=1e-5, momentum=0.1))
    return nn.Sequential(*layers)

class LayerNorm(nn.Module):# avg accross filters, H W, only batch dim stays
    # good workaround for RNN, but not so great for rest
    # u r not differentiating channels ( or normalizing them separately)
    __constants__ = ['eps']
    def __init__(self, eps=1e-5):
        super().__init__()
        self.eps = eps
        self.mult = nn.Parameter(tensor(1.))
        self.add  = nn.Parameter(tensor(0.))

    def forward(self, x):
        m = x.mean((1,2,3), keepdim=True) # every img has own mean, std
        v = x.var ((1,2,3), keepdim=True) # not avg across batch
        x = (x-m) / ((v+self.eps).sqrt()) # so no concept of running avg
        return x*self.mult + self.add

class InstanceNorm(nn.Module): # avg accross H W only
    #designed for style transfer not classification
    __constants__ = ['eps']
    def __init__(self, nf, eps=1e-0):
        super().__init__()
        self.eps = eps
        self.mults = nn.Parameter(torch.ones (nf,1,1))
        self.adds  = nn.Parameter(torch.zeros(nf,1,1))

    def forward(self, x):
        m = x.mean((2,3), keepdim=True) # avg accross H W only
        v = x.var ((2,3), keepdim=True) # avg accross H W only
        res = (x-m) / ((v+self.eps).sqrt())
        return res*self.mults + self.adds

class RunningBatchNorm_old(nn.Module):
    """We normalize by running moving avg, all the time, not just inference
    We use E[X^2] - E[X]^2, not averaging Var[X] """ # detail #1
    def __init__(self, nf, mom=0.1, eps=1e-5):
        super().__init__()
        self.mom,self.eps = mom,eps
        self.mults = nn.Parameter(torch.ones (nf,1,1))
        self.adds = nn.Parameter(torch.zeros(nf,1,1))
        self.register_buffer('sums', torch.zeros(1,nf,1,1)) #keeps buffer
        self.register_buffer('sqrs', torch.zeros(1,nf,1,1))
        self.register_buffer('batch', tensor(0.))
        self.register_buffer('count', tensor(0.)) # to adjust if bs iregular bs, detail #2
        self.register_buffer('step', tensor(0.))
        self.register_buffer('dbias', tensor(0.))

    def update_stats(self, x):
        bs,nc,*_ = x.shape
        self.sums.detach_()
        self.sqrs.detach_()
        dims = (0,2,3)
        s = x.sum(dims, keepdim=True)
        ss = (x*x).sum(dims, keepdim=True)
        c = self.count.new_tensor(x.numel()/nc) #bs*H*W ; what to divide sums, sqrs by #2
        mom1 = 1 - (1-self.mom)/math.sqrt(bs-1)
        self.mom1 = self.dbias.new_tensor(mom1)
        self.sums.lerp_(s, self.mom1)
        self.sqrs.lerp_(ss, self.mom1)
        self.count.lerp_(c, self.mom1)
        self.dbias = self.dbias*(1-self.mom1) + self.mom1
        self.batch += bs
        self.step += 1

    def forward(self, x):
        if self.training: self.update_stats(x) # only update stats in training
        sums = self.sums #update sums, sqrs and count to calc mvg avg
        sqrs = self.sqrs
        c = self.count
        if self.step<100:
            sums = sums / self.dbias # adjust for high influence of 1st points (zeros) in the mvg avg overtime
            sqrs = sqrs / self.dbias # detail # 3
            c    = c    / self.dbias
        means = sums/c  # but use moving avgs to normalize. always, not just inference time
        vars = (sqrs/c).sub_(means*means) # so no way affected in case latest mean or std is zero
        if bool(self.batch < 20): vars.clamp_min_(0.01) #avoid bad luck of very 1st points being zero
        x = (x-means).div_((vars.add_(self.eps)).sqrt())
        return x.mul_(self.mults).add_(self.adds)

    
class RunningBatchNorm(nn.Module):
    def __init__(self, nf, mom=0.1, eps=1e-5):
        super().__init__()
        self.mom, self.eps = mom, eps
        self.mults = nn.Parameter(torch.ones (nf,1,1))
        self.adds  = nn.Parameter(torch.zeros(nf,1,1))
        self.register_buffer('sums', torch.zeros(1,nf,1,1))
        self.register_buffer('sqrs', torch.zeros(1,nf,1,1))
        self.register_buffer('count', tensor(0.))
        self.register_buffer('factor', tensor(0.))
        self.register_buffer('offset', tensor(0.))
        self.batch = 0

    def update_stats(self, x):
        bs,nc,*_ = x.shape
        self.sums.detach_()
        self.sqrs.detach_()
        dims = (0,2,3)
        s    = x    .sum(dims, keepdim=True)
        ss   = (x*x).sum(dims, keepdim=True)
        c    = s.new_tensor(x.numel()/nc)
        mom1 = s.new_tensor(1 - (1-self.mom)/math.sqrt(bs-1))
        self.sums .lerp_(s , mom1)
        self.sqrs .lerp_(ss, mom1)
        self.count.lerp_(c , mom1)
        self.batch += bs
        means = self.sums/self.count
        varns = (self.sqrs/self.count).sub_(means*means)
        if bool(self.batch < 20): varns.clamp_min_(0.01)
        self.factor = self.mults / (varns+self.eps).sqrt()
        self.offset = self.adds - means*self.factor

    def forward(self, x):
        if self.training: self.update_stats(x)
        return x*self.factor + self.offset